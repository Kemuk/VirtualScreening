{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual Screening Results Analysis\n",
    "\n",
    "This notebook presents the results of the virtual screening campaign comparing:\n",
    "- **Vina**: Traditional docking score (lower = better binding)\n",
    "- **AEV-PLIG**: Machine learning rescoring (higher = better binding)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (10, 6),\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "})\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "MANIFEST_PATH = PROJECT_ROOT / 'data/master/manifest.parquet'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results'\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Manifest: {MANIFEST_PATH}\")\n",
    "print(f\"Results: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load manifest\n",
    "manifest = pq.read_table(MANIFEST_PATH).to_pandas()\n",
    "\n",
    "# Filter to rescored compounds\n",
    "df = manifest[manifest['rescoring_status'] == True].copy()\n",
    "if len(df) == 0:\n",
    "    print(\"Warning: No rescored compounds. Using docked compounds.\")\n",
    "    df = manifest[manifest['docking_status'] == True].copy()\n",
    "\n",
    "# Add convenience columns\n",
    "df['Vina'] = df['vina_score']\n",
    "df['AEV-PLIG'] = df['aev_plig_best_score']\n",
    "\n",
    "print(f\"Loaded {len(df):,} compounds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load computed metrics\n",
    "per_target = pd.read_csv(RESULTS_DIR / 'per_target_metrics.csv')\n",
    "summary = pd.read_csv(RESULTS_DIR / 'summary.csv')\n",
    "\n",
    "print(f\"Loaded metrics for {len(per_target)} targets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key statistics\n",
    "n_targets = df['protein_id'].nunique()\n",
    "n_compounds = len(df)\n",
    "n_actives = df['is_active'].sum()\n",
    "active_rate = 100 * n_actives / n_compounds\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "### Dataset Overview\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Targets | {n_targets} |\n",
    "| Compounds | {n_compounds:,} |\n",
    "| Actives | {n_actives:,} ({active_rate:.1f}%) |\n",
    "| Inactives | {n_compounds - n_actives:,} |\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary metrics table\n",
    "display(Markdown(\"### Performance Comparison\"))\n",
    "\n",
    "display_cols = ['Metric', 'Vina', 'AEV-PLIG', 'p_value', 'Significant']\n",
    "display_cols = [c for c in display_cols if c in summary.columns]\n",
    "display(summary[display_cols].style.set_caption(\"Median [95% CI] across targets\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Overall Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display ROC curves\n",
    "from IPython.display import Image\n",
    "\n",
    "roc_path = RESULTS_DIR / 'plots' / 'average_ROC.png'\n",
    "if roc_path.exists():\n",
    "    display(Markdown(\"### Average ROC Curves\"))\n",
    "    display(Image(filename=str(roc_path), width=600))\n",
    "else:\n",
    "    print(f\"ROC plot not found at {roc_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display PRC curves\n",
    "prc_path = RESULTS_DIR / 'plots' / 'average_PRC.png'\n",
    "if prc_path.exists():\n",
    "    display(Markdown(\"### Average Precision-Recall Curves\"))\n",
    "    display(Image(filename=str(prc_path), width=600))\n",
    "else:\n",
    "    print(f\"PRC plot not found at {prc_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Early Enrichment Analysis\n",
    "\n",
    "Early enrichment is the most important metric for virtual screening - it measures how well the method ranks actives at the top of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrichment factor comparison\n",
    "ef_metrics = ['EF1%', 'EF5%', 'EF10%']\n",
    "ef_data = []\n",
    "\n",
    "for metric in ef_metrics:\n",
    "    for method in ['Vina', 'AEV-PLIG']:\n",
    "        col = f'{method}_{metric}'\n",
    "        if col in per_target.columns:\n",
    "            values = per_target[col].dropna()\n",
    "            ef_data.append({\n",
    "                'Metric': metric,\n",
    "                'Method': method,\n",
    "                'Mean': values.mean(),\n",
    "                'Std': values.std(),\n",
    "            })\n",
    "\n",
    "ef_df = pd.DataFrame(ef_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(ef_metrics))\n",
    "width = 0.35\n",
    "\n",
    "vina_means = ef_df[ef_df['Method'] == 'Vina']['Mean'].values\n",
    "aevplig_means = ef_df[ef_df['Method'] == 'AEV-PLIG']['Mean'].values\n",
    "vina_stds = ef_df[ef_df['Method'] == 'Vina']['Std'].values\n",
    "aevplig_stds = ef_df[ef_df['Method'] == 'AEV-PLIG']['Std'].values\n",
    "\n",
    "ax.bar(x - width/2, vina_means, width, yerr=vina_stds, label='Vina', color='#3498db', capsize=5)\n",
    "ax.bar(x + width/2, aevplig_means, width, yerr=aevplig_stds, label='AEV-PLIG', color='#e74c3c', capsize=5)\n",
    "\n",
    "ax.axhline(y=1, color='gray', linestyle='--', label='Random')\n",
    "ax.set_ylabel('Enrichment Factor')\n",
    "ax.set_title('Early Enrichment Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(ef_metrics)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"actives in top 1%\" headline number\n",
    "display(Markdown(\"### Key Finding: Actives in Top 1%\"))\n",
    "\n",
    "for method in ['Vina', 'AEV-PLIG']:\n",
    "    col = f'{method}_EF1%'\n",
    "    if col in per_target.columns:\n",
    "        mean_ef = per_target[col].mean()\n",
    "        # EF1% = (actives in top 1%) / (expected actives in top 1%)\n",
    "        # So actives_found = EF * expected = EF * (n_actives * 0.01)\n",
    "        print(f\"{method}: {mean_ef:.1f}x more actives in top 1% than random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Per-Target Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of performance by target\n",
    "metrics_to_show = ['ROC-AUC', 'BEDROC', 'EF1%', 'NEF1%']\n",
    "\n",
    "# Build comparison dataframe\n",
    "comparison_data = []\n",
    "for _, row in per_target.iterrows():\n",
    "    target = row['Target']\n",
    "    for metric in metrics_to_show:\n",
    "        vina_col = f'Vina_{metric}'\n",
    "        aevplig_col = f'AEV-PLIG_{metric}'\n",
    "        if vina_col in row and aevplig_col in row:\n",
    "            vina_val = row[vina_col]\n",
    "            aevplig_val = row[aevplig_col]\n",
    "            if pd.notna(vina_val) and pd.notna(aevplig_val):\n",
    "                diff = aevplig_val - vina_val\n",
    "                comparison_data.append({\n",
    "                    'Target': target,\n",
    "                    'Metric': metric,\n",
    "                    'Difference': diff,  # Positive = AEV-PLIG better\n",
    "                })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "if len(comparison_df) > 0:\n",
    "    pivot = comparison_df.pivot(index='Target', columns='Metric', values='Difference')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, max(6, len(pivot) * 0.4)))\n",
    "    sns.heatmap(pivot, cmap='RdYlGn', center=0, annot=True, fmt='.2f', ax=ax)\n",
    "    ax.set_title('AEV-PLIG vs Vina (positive = AEV-PLIG better)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best and worst targets for AEV-PLIG\n",
    "display(Markdown(\"### Targets Where AEV-PLIG Performs Best\"))\n",
    "\n",
    "if 'AEV-PLIG_ROC-AUC' in per_target.columns and 'Vina_ROC-AUC' in per_target.columns:\n",
    "    per_target['AUC_improvement'] = per_target['AEV-PLIG_ROC-AUC'] - per_target['Vina_ROC-AUC']\n",
    "    \n",
    "    top_5 = per_target.nlargest(5, 'AUC_improvement')[['Target', 'N_Compounds', 'N_Actives', 'Vina_ROC-AUC', 'AEV-PLIG_ROC-AUC', 'AUC_improvement']]\n",
    "    display(top_5)\n",
    "    \n",
    "    display(Markdown(\"### Targets Where Vina Performs Better\"))\n",
    "    bottom_5 = per_target.nsmallest(5, 'AUC_improvement')[['Target', 'N_Compounds', 'N_Actives', 'Vina_ROC-AUC', 'AEV-PLIG_ROC-AUC', 'AUC_improvement']]\n",
    "    display(bottom_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Score Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score distributions for actives vs inactives\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Vina scores\n",
    "ax = axes[0]\n",
    "actives = df[df['is_active'] == True]['Vina'].dropna()\n",
    "inactives = df[df['is_active'] == False]['Vina'].dropna()\n",
    "\n",
    "ax.hist(inactives, bins=50, alpha=0.7, label=f'Inactives (n={len(inactives):,})', color='gray')\n",
    "ax.hist(actives, bins=50, alpha=0.7, label=f'Actives (n={len(actives):,})', color='green')\n",
    "ax.set_xlabel('Vina Score (kcal/mol)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Vina Score Distribution (lower = better)')\n",
    "ax.legend()\n",
    "\n",
    "# AEV-PLIG scores\n",
    "ax = axes[1]\n",
    "actives = df[df['is_active'] == True]['AEV-PLIG'].dropna()\n",
    "inactives = df[df['is_active'] == False]['AEV-PLIG'].dropna()\n",
    "\n",
    "ax.hist(inactives, bins=50, alpha=0.7, label=f'Inactives (n={len(inactives):,})', color='gray')\n",
    "ax.hist(actives, bins=50, alpha=0.7, label=f'Actives (n={len(actives):,})', color='green')\n",
    "ax.set_xlabel('AEV-PLIG Score')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('AEV-PLIG Score Distribution (higher = better)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on results\n",
    "display(Markdown(\"\"\"\n",
    "### Summary & Recommendations\n",
    "\n",
    "Based on the analysis above:\n",
    "\n",
    "1. **Overall Performance**: [To be filled based on results]\n",
    "\n",
    "2. **Early Enrichment**: [To be filled based on EF1% results]\n",
    "\n",
    "3. **Target-Specific Observations**: [To be filled based on per-target analysis]\n",
    "\n",
    "4. **Recommended Workflow**:\n",
    "   - Use Vina for initial docking\n",
    "   - Apply AEV-PLIG rescoring to improve ranking\n",
    "   - Select top N compounds for experimental validation\n",
    "\n",
    "5. **Suggested Score Cutoffs**:\n",
    "   - [To be determined based on ROC analysis]\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- [ ] Select top compounds for experimental testing\n",
    "- [ ] Validate predictions with binding assays\n",
    "- [ ] Iterate on model if needed\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
